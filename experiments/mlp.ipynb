{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "NRQ22DKC6tuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXFek3Sb5uep"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import os\n",
        "from PIL import Image\n",
        "from loguru import logger\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define paths\n",
        "save_path = \"/content/drive/MyDrive/saved\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Load CIFAR-10\n",
        "dataset = CIFAR10(root='./data', train=True, download=True)\n",
        "images = [dataset[i][0] for i in range(50000)]\n",
        "labels = [dataset[i][1] for i in range(50000)]\n",
        "labels = np.array(labels)\n",
        "\n",
        "split = int(0.7 * len(images))\n",
        "x_train, y_train = images[:split], labels[:split]\n",
        "x_test, y_test = images[split:], labels[split:]\n",
        "\n",
        "# Preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Feature extraction\n",
        "def extract_features(image_list):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "    model.eval().to(device)\n",
        "\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for img in image_list:\n",
        "            img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "            feat = model(img_tensor).squeeze().cpu().numpy()\n",
        "            features.append(feat)\n",
        "    return np.array(features)\n",
        "\n",
        "train_features = extract_features(x_train)\n",
        "test_features = extract_features(x_test)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train_features)\n",
        "test_scaled = scaler.transform(test_features)\n",
        "\n",
        "# Classifier\n",
        "clf = MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=5, verbose=True)\n",
        "clf.fit(train_scaled, y_train)\n",
        "\n",
        "# Save files\n",
        "joblib.dump(clf, f'{save_path}/mlp_cifar10.pkl')\n",
        "joblib.dump(scaler, f'{save_path}/scaler.pkl')\n",
        "np.save(f'{save_path}/train_features.npy', train_features)\n",
        "np.save(f'{save_path}/train_labels.npy', y_train)\n",
        "np.save(f'{save_path}/test_features.npy', test_features)\n",
        "np.save(f'{save_path}/test_labels.npy', y_test)\n",
        "\n",
        "train_images = np.stack([np.array(img) for img in x_train])\n",
        "test_images = np.stack([np.array(img) for img in x_test])\n",
        "np.save(f'{save_path}/train_images.npy', train_images)\n",
        "np.save(f'{save_path}/test_images.npy', test_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Load saved files\n",
        "base_path = \"/content/drive/MyDrive/saved\"\n",
        "clf = joblib.load(f\"{base_path}/mlp_cifar10.pkl\")\n",
        "scaler = joblib.load(f\"{base_path}/scaler.pkl\")\n",
        "train_features = np.load(f\"{base_path}/train_features.npy\")\n",
        "train_labels = np.load(f\"{base_path}/train_labels.npy\")\n",
        "train_images = np.load(f\"{base_path}/train_images.npy\")\n",
        "\n",
        "test_features = np.load(f\"{base_path}/test_features.npy\")\n",
        "test_labels = np.load(f\"{base_path}/test_labels.npy\")\n",
        "test_images = np.load(f\"{base_path}/test_images.npy\")\n",
        "\n",
        "top_k = 10\n",
        "\n",
        "correct = 0\n",
        "\n",
        "\n",
        "\n",
        "samples = random.sample(range(len(test_features)), 1000)\n",
        "\n",
        "for idx in samples:\n",
        "    query_img = test_images[idx]\n",
        "    query_feat = test_features[idx]\n",
        "    query_label = test_labels[idx]\n",
        "\n",
        "    query_feat_scaled = scaler.transform([query_feat])\n",
        "    pred_class = clf.predict(query_feat_scaled)[0]\n",
        "\n",
        "    class_indices = np.where(train_labels == pred_class)[0]\n",
        "    class_feats = train_features[class_indices]\n",
        "    class_imgs = train_images[class_indices]\n",
        "    class_labels = train_labels[class_indices]\n",
        "\n",
        "    sims = cosine_similarity([query_feat], class_feats)[0]\n",
        "    top_indices = np.argsort(sims)[-top_k:][::-1]\n",
        "    top_labels = class_labels[top_indices]\n",
        "\n",
        "    predicted_label = Counter(top_labels).most_common(1)[0][0]\n",
        "\n",
        "    if predicted_label == query_label:\n",
        "        correct += 1\n",
        "\n",
        "    if idx < 1000:\n",
        "        plt.figure(figsize=(15, 3))\n",
        "        plt.subplot(1, top_k + 1, 1)\n",
        "        plt.imshow(query_img)\n",
        "        plt.title(f\"Query\\nActual: {class_names[query_label]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        for j, top_idx in enumerate(top_indices):\n",
        "            img = class_imgs[top_idx]\n",
        "            lbl = class_labels[top_idx]\n",
        "            plt.subplot(1, top_k + 1, j + 2)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Top-{j+1}\\nLabel: {class_names[lbl]}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Predicted Class: {class_names[pred_class]} | Retrieved Label: {class_names[predicted_label]}\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "precision = correct / 1000\n",
        "recall = correct / len(samples)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f\"Precision@{top_k} over 1000 samples: {precision:.4f}\")\n",
        "print(f\" Recall@{top_k} over 1000 samples: {recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "Fbm3nIdL586f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}